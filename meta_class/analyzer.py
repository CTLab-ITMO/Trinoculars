import spacy
from collections import Counter
import math
import numpy as np

nlp = spacy.load("ru_core_news_lg")

def analyze_text(text):
    doc = nlp(text)

    tokens = [token.text for token in doc]
    words = [token.text for token in doc if token.is_alpha]
    unique_words = set(words)
    stop_words = [token.text for token in doc if token.is_stop]
    avg_word_length = sum(len(word) for word in words) / len(words) if words else 0

    return {
        "total_tokens": len(tokens),
        "total_words": len(words),
        "unique_words": len(unique_words),
        "stop_words": len(stop_words),
        "avg_word_length": round(avg_word_length, 2)
    }

if __name__ == "__main__":
    text = """
    –í —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–µ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å–∞ –∫ –≤–æ–ø—Ä–æ—Å–∞–º —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –º–∏—Ä–∞ —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ–π–∫—Å–∏—Å–∞ (–ù.–ê. –°–µ—Ä–µ–±—Ä—è–Ω—Å–∫–∞—è [1], –°.–ê. –ü—É—à–º–∏–Ω–∞ [2], –û.–ì. –ú–µ–ª—å–Ω–∏–∫ [3]). –î–µ–π–∫—Å–∏—Å —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∫–∞–∫ –æ–¥–Ω–æ –∏–∑ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø–æ–Ω—è—Ç–∏–π –ø—Ä–∞–≥–º–∞—Ç–∏–∫–∏. –í–∑–∞–∏–º–æ—Å–≤—è–∑—å –ø—Ä–∞–≥–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏—è —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Ç–∞–∫–∂–µ –∏–º–µ–µ—Ç –±–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–∏—Ç—É–∞—Ü–∏–∏ –æ–±—â–µ–Ω–∏—è. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–µ–π–∫—Å–∏—Å–∞, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–µ –î–∂. –õ–∞–π–æ–Ω–∑–æ–º, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç —ç—Ç—É –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å: ¬´–ü–æ–¥ –¥–µ–π–∫—Å–∏—Å–æ–º –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç—Å—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –ª–∏—Ü, –æ–±—ä–µ–∫—Ç–æ–≤, —Å–æ–±—ã—Ç–∏–π, –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏ –¥–µ–π—Å—Ç–≤–∏–π, –æ –∫–æ—Ç–æ—Ä—ã—Ö –≥–æ–≤–æ—Ä–∏—Ç—Å—è –∏–ª–∏ –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ —Å—Å—ã–ª–∞—é—Ç—Å—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Å–æ–∑–¥–∞–≤–∞–µ–º–æ–≥–æ –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–≥–æ –∞–∫—Ç–æ–º –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏—è –∏ —É—á–∞—Å—Ç–∏–µ–º –≤ –Ω–µ–º, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –æ–¥–Ω–æ–≥–æ –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –∏, –ø–æ –∫—Ä–∞–π–Ω–µ–π –º–µ—Ä–µ, –æ–¥–Ω–æ–≥–æ –∞–¥—Ä–µ—Å–∞—Ç–∞ [4. –°. 539]. 
    """
    
    analysis = analyze_text(text)
    
    print(f"\nüìä –ü–û–õ–ù–´–ô –ê–ù–ê–õ–ò–ó –¢–ï–ö–°–¢–ê")
    print(f"\n=== –ë–ê–ó–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===")
    print(f"- –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {analysis['total_tokens']}")
    print(f"- –í—Å–µ–≥–æ —Å–ª–æ–≤: {analysis['total_words']}")
    print(f"- –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {analysis['unique_words']}")
    print(f"- –°—Ç–æ–ø-—Å–ª–æ–≤: {analysis['stop_words']}")
    print(f"- –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞: {analysis['avg_word_length']} —Å–∏–º–≤–æ–ª–æ–≤")
